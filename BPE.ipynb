{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8e6b344",
   "metadata": {},
   "source": [
    "# Byte-Pair encoding \n",
    "\n",
    "Byte Pair Encoding (BPE) is a subword tokenization algorithm widely used in Natural Language Processing (NLP), particularly in large language models like GPT, BERT, and RoBERTa. It addresses the challenges of out-of-vocabulary words and large vocabulary sizes by representing words as sequences of smaller, meaningful units called subwords.\n",
    "\n",
    "How BPE Works:\n",
    "1. Initialization: The process begins by considering each unique character in the training text as an initial token in the vocabulary.\n",
    "\n",
    "2. Iterative Merging: The core of BPE involves repeatedly finding and merging the most frequent adjacent pairs of tokens in the training data.\n",
    "The algorithm identifies the pair of characters or subwords that appear most frequently next to each other.\n",
    "This most frequent pair is then merged into a new, single token, and this new token is added to the vocabulary.\n",
    "All occurrences of the merged pair in the training data are replaced with the new token.\n",
    "\n",
    "3. Vocabulary Expansion: This merging process continues for a predefined number of iterations or until a desired vocabulary size is reached. Each merge adds a new, longer subword unit to the vocabulary.\n",
    "Tokenization of New Text: When new text needs to be tokenized, the same sequence of merges learned during training is applied to the new data. This allows BPE to effectively handle unseen words by breaking them down into their constituent subword units, which are already present in the vocabulary.\n",
    "\n",
    "Key Advantages of BPE:\n",
    "\n",
    "1. Handling Out-of-Vocabulary (OOV) words: BPE can represent rare or unknown words by decomposing them into smaller, known subword units, preventing the issue of completely unknown tokens.\n",
    "\n",
    "2. Reduced Vocabulary Size: By merging frequent character pairs, BPE can significantly reduce the overall vocabulary size compared to word-level tokenization, making models more efficient.\n",
    "\n",
    "3. Balance between Word and Character Level: It strikes a balance between character-level and word-level tokenization, capturing both semantic information from subwords and the ability to compose full words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dfe41a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1751d57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Corpus:\n",
      "This is the first document.\n",
      "This document is the second document.\n",
      "And this is the third one.\n",
      "Is this the first document?\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Corpus:\")\n",
    "for doc in corpus:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f96ebed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ',\n",
       " '.',\n",
       " '?',\n",
       " 'A',\n",
       " 'I',\n",
       " 'T',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'h',\n",
       " 'i',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize vocabulary with unique characters\n",
    "unique_chars = set()\n",
    "for doc in corpus:\n",
    "    for char in doc:\n",
    "        unique_chars.add(char)\n",
    "\n",
    "unique_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4940da9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " '.',\n",
       " '?',\n",
       " 'A',\n",
       " 'I',\n",
       " 'T',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'h',\n",
       " 'i',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = list(unique_chars)\n",
    "vocab.sort()\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "635a8dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Vocabulary:\n",
      "[' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>']\n",
      "Vocabulary Size: 20\n"
     ]
    }
   ],
   "source": [
    "end_of_word = \"</w>\"\n",
    "vocab.append(end_of_word)\n",
    "\n",
    "print(\"Initial Vocabulary:\")\n",
    "print(vocab)\n",
    "print(f\"Vocabulary Size: {len(vocab)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21a1e287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pre-tokenized Word Frequencies:\n",
      "{('T', 'h', 'i', 's', '</w>'): 2, ('i', 's', '</w>'): 3, ('t', 'h', 'e', '</w>'): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '.', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('t', 'h', 'i', 's', '</w>'): 2, ('t', 'h', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's', '</w>'): 1, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '?', '</w>'): 1}\n"
     ]
    }
   ],
   "source": [
    "# Pre-tokenize the corpus: Split into words and then characters\n",
    "# We'll split by space for simplicity and add the end-of-word token\n",
    "word_splits = {}\n",
    "for doc in corpus:\n",
    "    words = doc.split(' ')\n",
    "    for word in words:\n",
    "        if word:\n",
    "            char_list = list(word) + [end_of_word]\n",
    "            # Use tuple for immutability if storing counts later - you can't change tuple once it's created (values, order, adding, removing elements, etc.), so they can be used as dictionary keys because of that.\n",
    "            word_tuple = tuple(char_list)\n",
    "            if word_tuple not in word_splits:\n",
    "                 word_splits[word_tuple] = 0\n",
    "            word_splits[word_tuple] += 1 # Count frequency of each initial word split\n",
    "\n",
    "print(\"\\nPre-tokenized Word Frequencies:\")\n",
    "print(word_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0ff533",
   "metadata": {},
   "source": [
    "Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c44a4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f37858f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pair_stats(splits):\n",
    "    \"\"\"Counts the frequency of adjacent pairs in the word splits.\"\"\"\n",
    "    # Initialize a dictionary with default values of 0 to count pairs of symbols.\n",
    "    # defaultdict: It's like a regular dictionary (dict), but with a key difference.\n",
    "    # If you try to access or modify a key that doesn't exist, instead of raising a KeyError,\n",
    "    # it automatically creates that key and assigns it a default value.\n",
    "    # int: This is the \"default factory\" you provide when creating the defaultdict. When a new key is created, it needs a default value, defaultdict calls this factory function. int() called with no arguments returns 0.\n",
    "    pair_counts = collections.defaultdict(int)\n",
    "    for word_tuple , freq in splits.items():\n",
    "        symbols = list(word_tuple)\n",
    "        for i in range(len(symbols)-1):\n",
    "            pair = (symbols[i], symbols[i+1])\n",
    "            pair_counts[pair] += freq \n",
    "    \n",
    "    return pair_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f6298d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {('T', 'h'): 2,\n",
       "             ('h', 'i'): 5,\n",
       "             ('i', 's'): 7,\n",
       "             ('s', '</w>'): 8,\n",
       "             ('t', 'h'): 7,\n",
       "             ('h', 'e'): 4,\n",
       "             ('e', '</w>'): 4,\n",
       "             ('f', 'i'): 2,\n",
       "             ('i', 'r'): 3,\n",
       "             ('r', 's'): 2,\n",
       "             ('s', 't'): 2,\n",
       "             ('t', '</w>'): 3,\n",
       "             ('d', 'o'): 4,\n",
       "             ('o', 'c'): 4,\n",
       "             ('c', 'u'): 4,\n",
       "             ('u', 'm'): 4,\n",
       "             ('m', 'e'): 4,\n",
       "             ('e', 'n'): 4,\n",
       "             ('n', 't'): 4,\n",
       "             ('t', '.'): 2,\n",
       "             ('.', '</w>'): 3,\n",
       "             ('s', 'e'): 1,\n",
       "             ('e', 'c'): 1,\n",
       "             ('c', 'o'): 1,\n",
       "             ('o', 'n'): 2,\n",
       "             ('n', 'd'): 2,\n",
       "             ('d', '</w>'): 3,\n",
       "             ('A', 'n'): 1,\n",
       "             ('r', 'd'): 1,\n",
       "             ('n', 'e'): 1,\n",
       "             ('e', '.'): 1,\n",
       "             ('I', 's'): 1,\n",
       "             ('t', '?'): 1,\n",
       "             ('?', '</w>'): 1})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_pair_stats(splits=word_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c083337d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pair(pair_to_merge, splits):\n",
    "    \"\"\"Merges the specified pair in the word splits.\"\"\"\n",
    "    new_splits = {}\n",
    "    (first, second) = pair_to_merge\n",
    "    merged_token = first + second\n",
    "    for word_tuple, freq in splits.items():\n",
    "        symbols = list(word_tuple)\n",
    "        new_symbols = []\n",
    "        i = 0\n",
    "        while i < len(symbols):\n",
    "            # If the current and next symbol match the pair to merge\n",
    "            if i < len(symbols) - 1 and symbols[i] == first and symbols[i+1] == second:\n",
    "                new_symbols.append(merged_token)\n",
    "                i += 2 # Skip the next symbol\n",
    "            else:\n",
    "                new_symbols.append(symbols[i])\n",
    "                i += 1\n",
    "        new_splits[tuple(new_symbols)] = freq # Use the updated symbol list as the key\n",
    "    return new_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd76332e",
   "metadata": {},
   "source": [
    " Iterative BPE Merging Loop\n",
    "\n",
    "Now we perform the core BPE training. We'll loop for a fixed number of merges (`num_merges`). In each iteration:\n",
    "1. Calculate the frequencies of all adjacent pairs in the current word representations using `get_pair_stats`.\n",
    "2. Find the pair with the highest frequency (`best_pair`).\n",
    "3. Merge this `best_pair` across all word representations using `merge_pair`.\n",
    "4. Add the newly formed token (concatenation of `best_pair`) to our vocabulary (`vocab`).\n",
    "5. Store the merge rule (mapping the pair to the new token) in the `merges` dictionary.\n",
    "\n",
    "We'll add print statements to observe the state at each step of the loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cf7ab747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting BPE Merges ---\n",
      "Initial Splits: {('T', 'h', 'i', 's', '</w>'): 2, ('i', 's', '</w>'): 3, ('t', 'h', 'e', '</w>'): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '.', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('t', 'h', 'i', 's', '</w>'): 2, ('t', 'h', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's', '</w>'): 1, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '?', '</w>'): 1}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 1/15\n",
      "Top 5 Pair Frequencies: [(('s', '</w>'), 8), (('i', 's'), 7), (('t', 'h'), 7), (('h', 'i'), 5), (('h', 'e'), 4)]\n",
      "Found Best Pair: ('s', '</w>') with Frequency: 8\n",
      "Merging ('s', '</w>') into 's</w>'\n",
      "Splits after merge: {('T', 'h', 'i', 's</w>'): 2, ('i', 's</w>'): 3, ('t', 'h', 'e', '</w>'): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '.', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('t', 'h', 'i', 's</w>'): 2, ('t', 'h', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>']\n",
      "Updated Merges: {('s', '</w>'): 's</w>'}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 2/15\n",
      "Top 5 Pair Frequencies: [(('i', 's</w>'), 7), (('t', 'h'), 7), (('h', 'i'), 5), (('h', 'e'), 4), (('e', '</w>'), 4)]\n",
      "Found Best Pair: ('i', 's</w>') with Frequency: 7\n",
      "Merging ('i', 's</w>') into 'is</w>'\n",
      "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('t', 'h', 'e', '</w>'): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '.', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('t', 'h', 'is</w>'): 2, ('t', 'h', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>']\n",
      "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>'}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 3/15\n",
      "Top 5 Pair Frequencies: [(('t', 'h'), 7), (('h', 'is</w>'), 4), (('h', 'e'), 4), (('e', '</w>'), 4), (('d', 'o'), 4)]\n",
      "Found Best Pair: ('t', 'h') with Frequency: 7\n",
      "Merging ('t', 'h') into 'th'\n",
      "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('th', 'e', '</w>'): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '.', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th']\n",
      "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th'}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 4/15\n",
      "Top 5 Pair Frequencies: [(('th', 'e'), 4), (('e', '</w>'), 4), (('d', 'o'), 4), (('o', 'c'), 4), (('c', 'u'), 4)]\n",
      "Found Best Pair: ('th', 'e') with Frequency: 4\n",
      "Merging ('th', 'e') into 'the'\n",
      "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the', '</w>'): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '.', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the']\n",
      "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the'}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 5/15\n",
      "Top 5 Pair Frequencies: [(('the', '</w>'), 4), (('d', 'o'), 4), (('o', 'c'), 4), (('c', 'u'), 4), (('u', 'm'), 4)]\n",
      "Found Best Pair: ('the', '</w>') with Frequency: 4\n",
      "Merging ('the', '</w>') into 'the</w>'\n",
      "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '.', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>']\n",
      "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '</w>'): 'the</w>'}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 6/15\n",
      "Top 5 Pair Frequencies: [(('d', 'o'), 4), (('o', 'c'), 4), (('c', 'u'), 4), (('u', 'm'), 4), (('m', 'e'), 4)]\n",
      "Found Best Pair: ('d', 'o') with Frequency: 4\n",
      "Merging ('d', 'o') into 'do'\n",
      "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('do', 'c', 'u', 'm', 'e', 'n', 't', '.', '</w>'): 2, ('do', 'c', 'u', 'm', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('do', 'c', 'u', 'm', 'e', 'n', 't', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do']\n",
      "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '</w>'): 'the</w>', ('d', 'o'): 'do'}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 7/15\n",
      "Top 5 Pair Frequencies: [(('do', 'c'), 4), (('c', 'u'), 4), (('u', 'm'), 4), (('m', 'e'), 4), (('e', 'n'), 4)]\n",
      "Found Best Pair: ('do', 'c') with Frequency: 4\n",
      "Merging ('do', 'c') into 'doc'\n",
      "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('doc', 'u', 'm', 'e', 'n', 't', '.', '</w>'): 2, ('doc', 'u', 'm', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('doc', 'u', 'm', 'e', 'n', 't', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc']\n",
      "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '</w>'): 'the</w>', ('d', 'o'): 'do', ('do', 'c'): 'doc'}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 8/15\n",
      "Top 5 Pair Frequencies: [(('doc', 'u'), 4), (('u', 'm'), 4), (('m', 'e'), 4), (('e', 'n'), 4), (('n', 't'), 4)]\n",
      "Found Best Pair: ('doc', 'u') with Frequency: 4\n",
      "Merging ('doc', 'u') into 'docu'\n",
      "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('docu', 'm', 'e', 'n', 't', '.', '</w>'): 2, ('docu', 'm', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('docu', 'm', 'e', 'n', 't', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc', 'docu']\n",
      "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '</w>'): 'the</w>', ('d', 'o'): 'do', ('do', 'c'): 'doc', ('doc', 'u'): 'docu'}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 9/15\n",
      "Top 5 Pair Frequencies: [(('docu', 'm'), 4), (('m', 'e'), 4), (('e', 'n'), 4), (('n', 't'), 4), (('i', 'r'), 3)]\n",
      "Found Best Pair: ('docu', 'm') with Frequency: 4\n",
      "Merging ('docu', 'm') into 'docum'\n",
      "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('docum', 'e', 'n', 't', '.', '</w>'): 2, ('docum', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('docum', 'e', 'n', 't', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc', 'docu', 'docum']\n",
      "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '</w>'): 'the</w>', ('d', 'o'): 'do', ('do', 'c'): 'doc', ('doc', 'u'): 'docu', ('docu', 'm'): 'docum'}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 10/15\n",
      "Top 5 Pair Frequencies: [(('docum', 'e'), 4), (('e', 'n'), 4), (('n', 't'), 4), (('i', 'r'), 3), (('t', '</w>'), 3)]\n",
      "Found Best Pair: ('docum', 'e') with Frequency: 4\n",
      "Merging ('docum', 'e') into 'docume'\n",
      "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('docume', 'n', 't', '.', '</w>'): 2, ('docume', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('docume', 'n', 't', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc', 'docu', 'docum', 'docume']\n",
      "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '</w>'): 'the</w>', ('d', 'o'): 'do', ('do', 'c'): 'doc', ('doc', 'u'): 'docu', ('docu', 'm'): 'docum', ('docum', 'e'): 'docume'}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 11/15\n",
      "Top 5 Pair Frequencies: [(('docume', 'n'), 4), (('n', 't'), 4), (('i', 'r'), 3), (('t', '</w>'), 3), (('.', '</w>'), 3)]\n",
      "Found Best Pair: ('docume', 'n') with Frequency: 4\n",
      "Merging ('docume', 'n') into 'documen'\n",
      "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('documen', 't', '.', '</w>'): 2, ('documen', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('documen', 't', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc', 'docu', 'docum', 'docume', 'documen']\n",
      "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '</w>'): 'the</w>', ('d', 'o'): 'do', ('do', 'c'): 'doc', ('doc', 'u'): 'docu', ('docu', 'm'): 'docum', ('docum', 'e'): 'docume', ('docume', 'n'): 'documen'}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 12/15\n",
      "Top 5 Pair Frequencies: [(('documen', 't'), 4), (('i', 'r'), 3), (('t', '</w>'), 3), (('.', '</w>'), 3), (('d', '</w>'), 3)]\n",
      "Found Best Pair: ('documen', 't') with Frequency: 4\n",
      "Merging ('documen', 't') into 'document'\n",
      "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('document', '.', '</w>'): 2, ('document', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('document', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc', 'docu', 'docum', 'docume', 'documen', 'document']\n",
      "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '</w>'): 'the</w>', ('d', 'o'): 'do', ('do', 'c'): 'doc', ('doc', 'u'): 'docu', ('docu', 'm'): 'docum', ('docum', 'e'): 'docume', ('docume', 'n'): 'documen', ('documen', 't'): 'document'}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 13/15\n",
      "Top 5 Pair Frequencies: [(('i', 'r'), 3), (('.', '</w>'), 3), (('d', '</w>'), 3), (('T', 'h'), 2), (('h', 'is</w>'), 2)]\n",
      "Found Best Pair: ('i', 'r') with Frequency: 3\n",
      "Merging ('i', 'r') into 'ir'\n",
      "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'ir', 's', 't', '</w>'): 2, ('document', '.', '</w>'): 2, ('document', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'ir', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('document', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc', 'docu', 'docum', 'docume', 'documen', 'document', 'ir']\n",
      "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '</w>'): 'the</w>', ('d', 'o'): 'do', ('do', 'c'): 'doc', ('doc', 'u'): 'docu', ('docu', 'm'): 'docum', ('docum', 'e'): 'docume', ('docume', 'n'): 'documen', ('documen', 't'): 'document', ('i', 'r'): 'ir'}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 14/15\n",
      "Top 5 Pair Frequencies: [(('.', '</w>'), 3), (('d', '</w>'), 3), (('T', 'h'), 2), (('h', 'is</w>'), 2), (('f', 'ir'), 2)]\n",
      "Found Best Pair: ('.', '</w>') with Frequency: 3\n",
      "Merging ('.', '</w>') into '.</w>'\n",
      "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'ir', 's', 't', '</w>'): 2, ('document', '.</w>'): 2, ('document', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'ir', 'd', '</w>'): 1, ('o', 'n', 'e', '.</w>'): 1, ('I', 's</w>'): 1, ('document', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc', 'docu', 'docum', 'docume', 'documen', 'document', 'ir', '.</w>']\n",
      "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '</w>'): 'the</w>', ('d', 'o'): 'do', ('do', 'c'): 'doc', ('doc', 'u'): 'docu', ('docu', 'm'): 'docum', ('docum', 'e'): 'docume', ('docume', 'n'): 'documen', ('documen', 't'): 'document', ('i', 'r'): 'ir', ('.', '</w>'): '.</w>'}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 15/15\n",
      "Top 5 Pair Frequencies: [(('d', '</w>'), 3), (('T', 'h'), 2), (('h', 'is</w>'), 2), (('f', 'ir'), 2), (('ir', 's'), 2)]\n",
      "Found Best Pair: ('d', '</w>') with Frequency: 3\n",
      "Merging ('d', '</w>') into 'd</w>'\n",
      "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'ir', 's', 't', '</w>'): 2, ('document', '.</w>'): 2, ('document', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd</w>'): 1, ('A', 'n', 'd</w>'): 1, ('th', 'is</w>'): 2, ('th', 'ir', 'd</w>'): 1, ('o', 'n', 'e', '.</w>'): 1, ('I', 's</w>'): 1, ('document', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc', 'docu', 'docum', 'docume', 'documen', 'document', 'ir', '.</w>', 'd</w>']\n",
      "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '</w>'): 'the</w>', ('d', 'o'): 'do', ('do', 'c'): 'doc', ('doc', 'u'): 'docu', ('docu', 'm'): 'docum', ('docum', 'e'): 'docume', ('docume', 'n'): 'documen', ('documen', 't'): 'document', ('i', 'r'): 'ir', ('.', '</w>'): '.</w>', ('d', '</w>'): 'd</w>'}\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- BPE Training Loop Initialization ---\n",
    "num_merges = 15\n",
    "# Stores merge rules, e.g., {('a', 'b'): 'ab'}\n",
    "# Example: {('T', 'h'): 'Th'}\n",
    "merges = {}\n",
    "# Initial word splits: {('T', 'h', 'i', 's', '</w>'): 2, ('i', 's', '</w>'): 2, ...}\n",
    "current_splits = word_splits.copy() # Start with initial word splits\n",
    "\n",
    "print(\"\\n--- Starting BPE Merges ---\")\n",
    "print(f\"Initial Splits: {current_splits}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "for i in range(num_merges):\n",
    "    print(f\"\\nMerge Iteration {i+1}/{num_merges}\")\n",
    "\n",
    "    # 1. Calculate Pair Frequencies\n",
    "    pair_stats = get_pair_stats(current_splits)\n",
    "    if not pair_stats:\n",
    "        print(\"No more pairs to merge.\")\n",
    "        break\n",
    "    # Optional: Print top 5 pairs for inspection\n",
    "    sorted_pairs = sorted(pair_stats.items(), key=lambda item: item[1], reverse=True)\n",
    "    print(f\"Top 5 Pair Frequencies: {sorted_pairs[:5]}\")\n",
    "\n",
    "    # 2. Find Best Pair\n",
    "    # The 'max' function iterates over all key-value pairs in the 'pair_stats' dictionary\n",
    "    # The 'key=pair_stats.get' tells 'max' to use the frequency (value) for comparison, not the pair (key) itself\n",
    "    # This way, 'max' selects the pair with the highest frequency\n",
    "    best_pair = max(pair_stats, key=pair_stats.get)\n",
    "    best_freq = pair_stats[best_pair]\n",
    "    print(f\"Found Best Pair: {best_pair} with Frequency: {best_freq}\")\n",
    "\n",
    "    # 3. Merge the Best Pair\n",
    "    current_splits = merge_pair(best_pair, current_splits)\n",
    "    new_token = best_pair[0] + best_pair[1]\n",
    "    print(f\"Merging {best_pair} into '{new_token}'\")\n",
    "    print(f\"Splits after merge: {current_splits}\")\n",
    "\n",
    "    # 4. Update Vocabulary\n",
    "    vocab.append(new_token)\n",
    "    print(f\"Updated Vocabulary: {vocab}\")\n",
    "\n",
    "    # 5. Store Merge Rule\n",
    "    merges[best_pair] = new_token\n",
    "    print(f\"Updated Merges: {merges}\")\n",
    "\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fae6ac",
   "metadata": {},
   "source": [
    "Review Final Results\n",
    "\n",
    "After the loop finishes, we can examine the final state:\n",
    "- The learned merge rules (`merges`).\n",
    "- The final representation of words after merges (`current_splits`).\n",
    "- The complete vocabulary (`vocab`) containing initial characters and learned subword tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6b978cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- BPE Merges Complete ---\n",
      "Final Vocabulary Size: 35\n",
      "\n",
      "Learned Merges (Pair -> New Token):\n",
      "('s', '</w>') -> 's</w>'\n",
      "('i', 's</w>') -> 'is</w>'\n",
      "('t', 'h') -> 'th'\n",
      "('th', 'e') -> 'the'\n",
      "('the', '</w>') -> 'the</w>'\n",
      "('d', 'o') -> 'do'\n",
      "('do', 'c') -> 'doc'\n",
      "('doc', 'u') -> 'docu'\n",
      "('docu', 'm') -> 'docum'\n",
      "('docum', 'e') -> 'docume'\n",
      "('docume', 'n') -> 'documen'\n",
      "('documen', 't') -> 'document'\n",
      "('i', 'r') -> 'ir'\n",
      "('.', '</w>') -> '.</w>'\n",
      "('d', '</w>') -> 'd</w>'\n",
      "\n",
      "Final Word Splits after all merges:\n",
      "{('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'ir', 's', 't', '</w>'): 2, ('document', '.</w>'): 2, ('document', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd</w>'): 1, ('A', 'n', 'd</w>'): 1, ('th', 'is</w>'): 2, ('th', 'ir', 'd</w>'): 1, ('o', 'n', 'e', '.</w>'): 1, ('I', 's</w>'): 1, ('document', '?', '</w>'): 1}\n",
      "\n",
      "Final Vocabulary (sorted):\n",
      "[' ', '.', '.</w>', '</w>', '?', 'A', 'I', 'T', 'c', 'd', 'd</w>', 'do', 'doc', 'docu', 'docum', 'docume', 'documen', 'document', 'e', 'f', 'h', 'i', 'ir', 'is</w>', 'm', 'n', 'o', 'r', 's', 's</w>', 't', 'th', 'the', 'the</w>', 'u']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- BPE Merges Complete ---\")\n",
    "print(f\"Final Vocabulary Size: {len(vocab)}\")\n",
    "print(\"\\nLearned Merges (Pair -> New Token):\")\n",
    "# Pretty print merges\n",
    "for pair, token in merges.items():\n",
    "    print(f\"{pair} -> '{token}'\")\n",
    "\n",
    "print(\"\\nFinal Word Splits after all merges:\")\n",
    "print(current_splits)\n",
    "\n",
    "print(\"\\nFinal Vocabulary (sorted):\")\n",
    "# Sort for consistent viewing\n",
    "final_vocab_sorted = sorted(list(set(vocab))) # Use set to remove potential duplicates if any step introduced them\n",
    "print(final_vocab_sorted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35143b97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
