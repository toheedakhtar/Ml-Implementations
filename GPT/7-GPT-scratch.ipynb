{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tiktoken in /home/toheed/.local/lib/python3.10/site-packages (0.7.0)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/toheed/.local/lib/python3.10/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/toheed/.local/lib/python3.10/site-packages (from tiktoken) (2024.7.24)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.26.0->tiktoken) (1.26.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/toheed/.local/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.26.0->tiktoken) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.26.0->tiktoken) (2020.6.20)\n"
     ]
    }
   ],
   "source": [
    "! pip install tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-09-04 11:11:51--  https://raw.githubusercontent.com/jcjohnson/torch-rnn/master/data/tiny-shakespeare.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8002::154, 2606:50c0:8003::154, 2606:50c0:8000::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8002::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘tiny-shakespeare.txt.2’\n",
      "\n",
      "tiny-shakespeare.tx 100%[===================>]   1.06M   795KB/s    in 1.4s    \n",
      "\n",
      "2024-09-04 11:11:53 (795 KB/s) - ‘tiny-shakespeare.txt.2’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ! wget https://raw.githubusercontent.com/jcjohnson/torch-rnn/master/data/tiny-shakespeare.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset 1115394 \n",
      "\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "with open('./../tiny-shakespeare.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print('Length of dataset', len(text), '\\n')\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "\n",
      "Vocabulary size: 65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(f'\\nVocabulary size: {vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# encode and decoder\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "msg = 'hii there'\n",
    "token_list = encode(msg)\n",
    "print(token_list)\n",
    "print(decode(token_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Digression - TikToken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[71, 4178, 612]\n",
      "hii there\n",
      "50257\n"
     ]
    }
   ],
   "source": [
    "enc = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "msg = 'hii there'\n",
    "token_list = enc.encode(msg)\n",
    "print(token_list)\n",
    "print(enc.decode(enc.encode('hii there')))\n",
    "\n",
    "print(enc.n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size: torch.Size([1115394]) elements of type torch.int64\n",
      "First 10 token form dataset tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(f'Total size: {data.shape} elements of type {data.dtype}')\n",
    "print('First 10 token form dataset', data[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1003854\n"
     ]
    }
   ],
   "source": [
    "n = int(0.9 * len(data)) # split here 90%\n",
    "print(n)\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when the prompt is tensor([18]), predict is 47\n",
      "when the prompt is tensor([18, 47]), predict is 56\n",
      "when the prompt is tensor([18, 47, 56]), predict is 57\n",
      "when the prompt is tensor([18, 47, 56, 57]), predict is 58\n",
      "when the prompt is tensor([18, 47, 56, 57, 58]), predict is 1\n",
      "when the prompt is tensor([18, 47, 56, 57, 58,  1]), predict is 15\n",
      "when the prompt is tensor([18, 47, 56, 57, 58,  1, 15]), predict is 47\n",
      "when the prompt is tensor([18, 47, 56, 57, 58,  1, 15, 47]), predict is 58\n"
     ]
    }
   ],
   "source": [
    "# first block \n",
    "x = train_data[:block_size]\n",
    "# individual token shifted by one\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f'when the prompt is {context}, predict is {target}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # number of sequence to be processed in parallel (for gpu)\n",
    "block_size = 8 # length of sequence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs shape:  torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]]) \n",
      "\n",
      "targets shape:  torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]]) \n",
      "\n",
      "When the prompt is tensor([24]), predict 43\n",
      "When the prompt is tensor([24, 43]), predict 58\n",
      "When the prompt is tensor([24, 43, 58]), predict 5\n",
      "When the prompt is tensor([24, 43, 58,  5]), predict 57\n",
      "When the prompt is tensor([24, 43, 58,  5, 57]), predict 1\n",
      "When the prompt is tensor([24, 43, 58,  5, 57,  1]), predict 46\n",
      "When the prompt is tensor([24, 43, 58,  5, 57,  1, 46]), predict 43\n",
      "When the prompt is tensor([24, 43, 58,  5, 57,  1, 46, 43]), predict 39\n",
      "When the prompt is tensor([44]), predict 53\n",
      "When the prompt is tensor([44, 53]), predict 56\n",
      "When the prompt is tensor([44, 53, 56]), predict 1\n",
      "When the prompt is tensor([44, 53, 56,  1]), predict 58\n",
      "When the prompt is tensor([44, 53, 56,  1, 58]), predict 46\n",
      "When the prompt is tensor([44, 53, 56,  1, 58, 46]), predict 39\n",
      "When the prompt is tensor([44, 53, 56,  1, 58, 46, 39]), predict 58\n",
      "When the prompt is tensor([44, 53, 56,  1, 58, 46, 39, 58]), predict 1\n",
      "When the prompt is tensor([52]), predict 58\n",
      "When the prompt is tensor([52, 58]), predict 1\n",
      "When the prompt is tensor([52, 58,  1]), predict 58\n",
      "When the prompt is tensor([52, 58,  1, 58]), predict 46\n",
      "When the prompt is tensor([52, 58,  1, 58, 46]), predict 39\n",
      "When the prompt is tensor([52, 58,  1, 58, 46, 39]), predict 58\n",
      "When the prompt is tensor([52, 58,  1, 58, 46, 39, 58]), predict 1\n",
      "When the prompt is tensor([52, 58,  1, 58, 46, 39, 58,  1]), predict 46\n",
      "When the prompt is tensor([25]), predict 17\n",
      "When the prompt is tensor([25, 17]), predict 27\n",
      "When the prompt is tensor([25, 17, 27]), predict 10\n",
      "When the prompt is tensor([25, 17, 27, 10]), predict 0\n",
      "When the prompt is tensor([25, 17, 27, 10,  0]), predict 21\n",
      "When the prompt is tensor([25, 17, 27, 10,  0, 21]), predict 1\n",
      "When the prompt is tensor([25, 17, 27, 10,  0, 21,  1]), predict 54\n",
      "When the prompt is tensor([25, 17, 27, 10,  0, 21,  1, 54]), predict 39\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split, batch_size):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    # Tensor of shape (batch_size,) with random sequence start indices between 0 and len(data) - block_size\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "\n",
    "    # Accumulate and add each sequence of this batch to form a tensor\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    # Same as x but shifted by one token\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x , y\n",
    "\n",
    "xb , yb = get_batch('train', batch_size)\n",
    "\n",
    "print('inputs shape: ', xb.shape)\n",
    "print(xb,'\\n')\n",
    "print('targets shape: ', yb.shape)\n",
    "print(yb, '\\n')\n",
    "\n",
    "# print first batch \n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f'When the prompt is {context}, predict {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 65\n",
      "torch.Size([4, 8, 65])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # embedding the vocabulary\n",
    "        # Every one of the vocab_size tokens is represented by a vector of size vocab_size\n",
    "        self.embed = nn.Embedding(vocab_size, vocab_size) # 65 unique 65-dim vectors\n",
    "    \n",
    "    def forward(self, idx, targets):\n",
    "        # idx - shape (batch_size, block_size)\n",
    "        # targets - shape (batch_size, block_size)\n",
    "        # Embed the input indices, shape is now (batch_size, block_size, vocab_size) (B, T, C)\n",
    "        logits = self.embed(idx)\n",
    "        return logits\n",
    "    \n",
    "print('Vocabulary size:', vocab_size)  # Length of the vocabulary list (this includes the space character)\n",
    "m = BigramLM(vocab_size)  # Instantiate the model\n",
    "out = m.forward(xb, yb)           # Forward pass (yb remains unused for now)\n",
    "print(out.shape)    # (batch_size, block_size, vocab_size) -> 4 times 8 characters, each embedded as a 65-dim vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 65\n",
      "torch.Size([32, 65])\n",
      "4.878634929656982\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # embedding the vocabulary\n",
    "        # Every one of the vocab_size tokens is represented by a vector of size vocab_size\n",
    "        self.embed = nn.Embedding(vocab_size, vocab_size) # 65 unique 65-dim vectors\n",
    "    \n",
    "    def forward(self, idx, targets):\n",
    "        logits = self.embed(idx)      # Embed the input indices, shape is now (batch_size, block_size, vocab_size) (B, T, C)\n",
    "        B, T, C = logits.shape        # B = batch_size, T = block_size, C = vocab_size\n",
    "        logits = logits.view(B*T, C)  # Transpose logits to (B*T, C)\n",
    "        # This is the first time we actively use the targets:\n",
    "        targets = targets.view(B*T)   # Transpose targets to (B*T) (targets contains the next token's index for each input sequence in the batch)\n",
    "        loss = F.cross_entropy(logits, targets)  # Calculating cross entropy loss across all tokens in the batch (using targets to plug out the correct token for each input sequence)\n",
    "        return logits, loss\n",
    "\n",
    "    \n",
    "print('Vocabulary size:', vocab_size)  # Length of the vocabulary list (this includes the space character)\n",
    "m = BigramLM(vocab_size)  # Instantiate the model\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> adding genrate() to BigramLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 65\n",
      "torch.Size([32, 65]) \n",
      " tensor(4.8786, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # embedding the vocabulary\n",
    "        # Every one of the vocab_size tokens is represented by a vector of size vocab_size\n",
    "        self.embed = nn.Embedding(vocab_size, vocab_size) # 65 unique 65-dim vectors\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.embed(idx)                               # Embed the input indices, shape is now (batch_size, block_size, vocab_size) (B, T, C)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)                       # Transpose logits to (B, C, T) (B=batch_size, T=block_size, C=vocab_size)\n",
    "            targets = targets.view(B*T)                        # Transpose targets to (B, T)\n",
    "            loss = F.cross_entropy(logits, targets)            # Calculating cross entropy loss across all tokens in the batch\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = self(idx)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "   \n",
    "print('Vocabulary size:', vocab_size)  # Length of the vocabulary list (this includes the space character)\n",
    "m = BigramLM(vocab_size)  # Instantiate \n",
    "logits, loss = m(xb, yb)\n",
    "\n",
    "print(logits.shape, '\\n', loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Producing First Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 101])\n",
      "\n",
      "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
     ]
    }
   ],
   "source": [
    "ix = torch.zeros((1, 1), dtype=torch.long)  # Start with a single tensor of shape (1, 1) holding a 0 (new line)\n",
    "tokens = m.generate(ix, max_new_tokens=100) # Generate 100 tokens as a sequence of indices\n",
    "print(tokens.shape)                         # Print the shape of the resulting sequence of tokens\n",
    "print(decode(tokens[0].tolist()))           # Decode the resulting sequence of indices to a string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at steps 0: 4.704006195068359\n",
      "Loss at steps 100: 4.658433437347412\n",
      "Loss at steps 200: 4.470171928405762\n",
      "Loss at steps 300: 4.320702075958252\n",
      "Loss at steps 400: 4.252743721008301\n",
      "Loss at steps 500: 4.241008758544922\n",
      "Loss at steps 600: 4.161406517028809\n",
      "Loss at steps 700: 4.044336795806885\n",
      "Loss at steps 800: 4.091874122619629\n",
      "Loss at steps 900: 3.7458465099334717\n",
      "Loss at steps 1000: 3.7031264305114746\n",
      "Loss at steps 1100: 3.7115283012390137\n",
      "Loss at steps 1200: 3.6330997943878174\n",
      "Loss at steps 1300: 3.422212600708008\n",
      "Loss at steps 1400: 3.4295449256896973\n",
      "Loss at steps 1500: 3.4233598709106445\n",
      "Loss at steps 1600: 3.3018524646759033\n",
      "Loss at steps 1700: 3.283510446548462\n",
      "Loss at steps 1800: 3.188281774520874\n",
      "Loss at steps 1900: 3.2000553607940674\n",
      "Loss at steps 2000: 3.1371781826019287\n",
      "Loss at steps 2100: 3.0028276443481445\n",
      "Loss at steps 2200: 3.058077812194824\n",
      "Loss at steps 2300: 2.958632707595825\n",
      "Loss at steps 2400: 2.981365919113159\n",
      "Loss at steps 2500: 2.9196817874908447\n",
      "Loss at steps 2600: 2.8414011001586914\n",
      "Loss at steps 2700: 2.8905837535858154\n",
      "Loss at steps 2800: 2.9735329151153564\n",
      "Loss at steps 2900: 2.808624029159546\n",
      "Loss at steps 3000: 2.776794672012329\n",
      "Loss at steps 3100: 2.748556137084961\n",
      "Loss at steps 3200: 2.687368392944336\n",
      "Loss at steps 3300: 2.682086944580078\n",
      "Loss at steps 3400: 2.688863754272461\n",
      "Loss at steps 3500: 2.809856414794922\n",
      "Loss at steps 3600: 2.6931402683258057\n",
      "Loss at steps 3700: 2.665353298187256\n",
      "Loss at steps 3800: 2.632939100265503\n",
      "Loss at steps 3900: 2.75382924079895\n",
      "Loss at steps 4000: 2.5844571590423584\n",
      "Loss at steps 4100: 2.630505323410034\n",
      "Loss at steps 4200: 2.6259851455688477\n",
      "Loss at steps 4300: 2.5507774353027344\n",
      "Loss at steps 4400: 2.5834596157073975\n",
      "Loss at steps 4500: 2.6057393550872803\n",
      "Loss at steps 4600: 2.6198649406433105\n",
      "Loss at steps 4700: 2.5730929374694824\n",
      "Loss at steps 4800: 2.5133121013641357\n",
      "Loss at steps 4900: 2.6088051795959473\n",
      "Loss at steps 5000: 2.5105180740356445\n",
      "Loss at steps 5100: 2.574131727218628\n",
      "Loss at steps 5200: 2.4975621700286865\n",
      "Loss at steps 5300: 2.525254487991333\n",
      "Loss at steps 5400: 2.4852945804595947\n",
      "Loss at steps 5500: 2.548015594482422\n",
      "Loss at steps 5600: 2.577272415161133\n",
      "Loss at steps 5700: 2.602153778076172\n",
      "Loss at steps 5800: 2.3584301471710205\n",
      "Loss at steps 5900: 2.448211908340454\n",
      "Loss at steps 6000: 2.531585931777954\n",
      "Loss at steps 6100: 2.497115135192871\n",
      "Loss at steps 6200: 2.446481943130493\n",
      "Loss at steps 6300: 2.4474494457244873\n",
      "Loss at steps 6400: 2.5307929515838623\n",
      "Loss at steps 6500: 2.4708240032196045\n",
      "Loss at steps 6600: 2.5558059215545654\n",
      "Loss at steps 6700: 2.469895839691162\n",
      "Loss at steps 6800: 2.605414867401123\n",
      "Loss at steps 6900: 2.570693254470825\n",
      "Loss at steps 7000: 2.504757881164551\n",
      "Loss at steps 7100: 2.444789171218872\n",
      "Loss at steps 7200: 2.5223405361175537\n",
      "Loss at steps 7300: 2.4506444931030273\n",
      "Loss at steps 7400: 2.410496234893799\n",
      "Loss at steps 7500: 2.400172472000122\n",
      "Loss at steps 7600: 2.4295127391815186\n",
      "Loss at steps 7700: 2.424211025238037\n",
      "Loss at steps 7800: 2.494037389755249\n",
      "Loss at steps 7900: 2.438490390777588\n",
      "Loss at steps 8000: 2.4696712493896484\n",
      "Loss at steps 8100: 2.4945294857025146\n",
      "Loss at steps 8200: 2.4096617698669434\n",
      "Loss at steps 8300: 2.6117146015167236\n",
      "Loss at steps 8400: 2.4675140380859375\n",
      "Loss at steps 8500: 2.4209394454956055\n",
      "Loss at steps 8600: 2.4633748531341553\n",
      "Loss at steps 8700: 2.4416561126708984\n",
      "Loss at steps 8800: 2.634512186050415\n",
      "Loss at steps 8900: 2.3378663063049316\n",
      "Loss at steps 9000: 2.4838879108428955\n",
      "Loss at steps 9100: 2.534738779067993\n",
      "Loss at steps 9200: 2.58534574508667\n",
      "Loss at steps 9300: 2.3732097148895264\n",
      "Loss at steps 9400: 2.4463818073272705\n",
      "Loss at steps 9500: 2.407996892929077\n",
      "Loss at steps 9600: 2.525811195373535\n",
      "Loss at steps 9700: 2.521479845046997\n",
      "Loss at steps 9800: 2.4382166862487793\n",
      "Loss at steps 9900: 2.4321024417877197\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "losses = []\n",
    "\n",
    "for steps in range(10000):\n",
    "    xb, yb = get_batch('train', batch_size)     # sample batch of data\n",
    "    logits, loss = m(xb, yb)                    # forward pass\n",
    "    loss.backward()                             # Backdrop with pytorch autograd (updt. logits - the embedding vector)\n",
    "\n",
    "    opt.step()                                   # update weights\n",
    "    opt.zero_grad()                              # Set gradients to zero\n",
    "\n",
    "    if steps % 100 == 0:\n",
    "        print(f'Loss at steps {steps}: {loss.item()}')\n",
    "        losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iyoteng h hasbe pave pirance\n",
      "Rie hicomyonthar's\n",
      "Plinseard ith henoure wounonthioneir thondy, y heltieiengerofo'dsssit ey\n",
      "KIN d pe wither vouprrouthercc.\n",
      "hathe; d!\n",
      "My hind tt hinig t ouchos tes; st yo hind wotte grotonear 'so it t jod weancotha:\n",
      "h hay.JUCle n prids, r loncave w hollular s O:\n",
      "HIs; ht anjx?\n",
      "\n",
      "DUThinqunt.\n",
      "\n",
      "LaZAnde.\n",
      "athave l.\n",
      "KEONH:\n",
      "ARThanco be y,-hedarwnoddy scace, tridesar, wnl'shenous s ls, theresseys\n",
      "PlorseelapinghiybHen yof GLUCEN t l-t E:\n",
      "I hisgothers je are!-e!\n",
      "QLYotouciullle'z,\n",
      "Thitertho s?\n",
      "NDan'spererfo cist ripl chys er orlese;\n",
      "Yo jehof h hecere ek? wferommot mowo soaf yoit, ince his, t, f at. fal whetrimy bupof tor atha Bu!\n",
      "JOutho f cimimave.\n",
      "NEDUSt cir selle p wie wede\n",
      "Ro n apenor f'Y tover witys an sh d w t e w!\n",
      "CEOntiretoaveE IINpe, theck. cung.\n",
      "ORIsthies hacin benqurd bll, d a r w wistatsowor ath\n",
      "Fivet bloll ang a-I theeancu,\n",
      "LINCI'T:\n",
      "Sarry t I Ane sze t\n",
      "LCKI thit,\n",
      "n.\n",
      "Faure ds ppplirn!\n",
      "meftou ow pring, avewist th;\n",
      "TENTEMETCI gienco, An he waro whiougou he s im\n"
     ]
    }
   ],
   "source": [
    "# sampling from model \n",
    "print(decode(m.generate(torch.zeros((1,1,), dtype=torch.long), max_new_tokens=1000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x74757972eb60>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mathematical trick in Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch [0]:\n",
      " tensor([[ 0.1808, -0.0700],\n",
      "        [-0.3596, -0.9152],\n",
      "        [ 0.6258,  0.0255],\n",
      "        [ 0.9545,  0.0643],\n",
      "        [ 0.3612,  1.1679],\n",
      "        [-1.3499, -0.5102],\n",
      "        [ 0.2360, -0.2398],\n",
      "        [-0.9211,  1.5433]]) \n",
      "\n",
      "Running Average:\n",
      " tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]])\n"
     ]
    }
   ],
   "source": [
    "xbow = torch.zeros((B,T,C))         # Create tensor of zeros of shape (B, T, C) \n",
    "for b in range(B):                       # For all batches\n",
    "    for t in range(T):                  #  For all tokens in the batch  \n",
    "        xprev = x[b, :t+1]             # Get all tokens up to and including the current token \n",
    "        xbow[b,t] = xprev.mean(dim=0)  # Calculate the mean of the tokens up to and including the current token\n",
    "\n",
    "print('Batch [0]:\\n', x[0], '\\n')\n",
    "print('Running Average:\\n', xbow[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a (ones) =\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "\n",
      "b (random) =\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "\n",
      "c = a @ b =\n",
      "tensor([[14., 16.],\n",
      "        [14., 16.],\n",
      "        [14., 16.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# loops are inefficent so , \n",
    "torch.manual_seed(42)\n",
    "a = torch.ones(3,3)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "\n",
    "print(f'a (ones) =\\n{a}\\n')\n",
    "print(f'b (random) =\\n{b}\\n')\n",
    "print(f'c = a @ b =\\n{c}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a (ones + tril) =\n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "\n",
      "b (random) =\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "\n",
      "c = a @ b =\n",
      "tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))            # Lower triangular matrix of ones (tril used here)\n",
    "b = torch.randint(0, 10, (3, 2)).float()    # 3x2 matrix of random integers between 0 and 9\n",
    "c = a @ b                                   # Matrix multiplication of a and b\n",
    "\n",
    "print(f'a (ones + tril) =\\n{a}\\n')\n",
    "print(f'b (random) =\\n{b}\\n')\n",
    "print(f'c = a @ b =\\n{c}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a (ones + tril + avg) =\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "\n",
      "b (random) =\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "\n",
      "c = a @ b =\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))   # Lower triangular matrix of ones\n",
    "a = a / a.sum(dim=1, keepdim=True) # Normalize the matrix by dividing along each row\n",
    "b = torch.randint(0, 10, (3, 2)).float() # 3x2 matrix of random integers between 0 and 9\n",
    "c = a @ b                                # Matrix multiplication of a and b\n",
    "\n",
    "print(f'a (ones + tril + avg) =\\n{a}\\n')\n",
    "print(f'b (random) =\\n{b}\\n')\n",
    "print(f'c = a @ b =\\n{c}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# applying this to our context, we can get the running average of the tokens in the batch \n",
    "# by multiplying the batch with torch.tril(a)\n",
    "\n",
    "B, T, C = 4, 8 , 2\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# old \n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1]\n",
    "        xbow[b,t] = xprev.mean(dim = 0)\n",
    "    \n",
    "# new \n",
    "wei = torch.tril(torch.ones(T,T))\n",
    "wei = wei / wei.sum(dim=1, keepdim=True)\n",
    "xbow2 = wei @ x     # (T, T) @ (B, T, C) -> PyTorch's Auto-Stride -> (B, T, T) @ (B, T, C) = (B, T, C)\n",
    "\n",
    "\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adding softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# New:\n",
    "wei = torch.tril(torch.ones(T, T))       # Lower triangular matrix of ones\n",
    "wei = wei / wei.sum(dim=1, keepdim=True) # Normalizing wei by dividing by the sum of each row\n",
    "xbow2 = wei @ x                          # (T, T) @ (B, T, C) -> (B, T, T) @ (B, T, C) = (B, T, C)\n",
    "\n",
    "# Newer:\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "\n",
    "wei = torch.zeros((T,T))\n",
    "# print(wei)\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "# print(wei)\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow2, xbow3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf]])\n",
      "tensor([[0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "# row wise softmax\n",
    "\n",
    "exwei = torch.tensor([[0, 0, float('-inf'), float('-inf'), float('-inf'), float('-inf'), float('-inf')],  [0, 0, 0, float('-inf'), float('-inf'), float('-inf'), float('-inf')]])\n",
    "print(exwei)\n",
    "exsof = F.softmax(exwei, dim=-1) # -1 means the last dimension\n",
    "print(exsof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T, T))             # Lower triangular matrix of ones\n",
    "wei = torch.zeros((T, T))                       # (T, T)\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # Masking all values in wei where tril == 0 with -inf\n",
    "wei = F.softmax(wei, dim=-1)                    # (T, T)\n",
    "xbow3 = wei @ x                                 # (T, T) @ (B, T, C) -> (B, T, T) @ (B, T, C) = (B, T, C)\n",
    "\n",
    "print(wei[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self - Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 16])\n",
      "torch.Size([4, 16, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 8])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key = nn.Linear(C, 16, bias=False)\n",
    "query = nn.Linear(C, 16, bias=False)\n",
    "k = key(x)\n",
    "q = query(x)\n",
    "print(q.shape)\n",
    "k = k.transpose(-2, -1)\n",
    "print(k.shape)\n",
    "wei = q @ k\n",
    "wei.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4,8,32    # batch, Time. Channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# Single Head Self Attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "# value = nn.Linear(C, head)\n",
    "k = key(x)      # (B, T, 16)    \n",
    "q = query(x)    # (B, T, 16)     \n",
    "wei = q @ k.transpose(-2, -1)   # (B, T, 16) @ (B, 16, T) --> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "\n",
    "out = wei @ v\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaled Dot product attention  \n",
    "handles unit variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.randn(B, T, head_size)\n",
    "q = torch.randn(B, T, head_size)\n",
    "wei = q @ k.transpose(-2, -1) * head_size ** -0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9487)\n",
      "tensor(1.0449)\n",
      "tensor(0.8980)\n"
     ]
    }
   ],
   "source": [
    "print(k.var())\n",
    "print(q.var())\n",
    "print(wei.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
